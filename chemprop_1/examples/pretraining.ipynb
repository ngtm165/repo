{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression - Reaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labhhc2/Documents/workspace/D20/Tam/repo/chemprop_1/examples\n",
      "/home/labhhc2/Documents/workspace/D20/Tam/repo/chemprop_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_path=os.getcwd()\n",
    "print(current_path)\n",
    "\n",
    "parent_path=os.path.dirname(current_path)\n",
    "print(parent_path)\n",
    "\n",
    "if parent_path not in sys.path:\n",
    "    sys.path.append(parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "from pathlib import Path\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change data inputs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chemprop_dir = Path.cwd().parent\n",
    "num_workers = 20  # number of workers for dataloader. 0 means using main process for data loading\n",
    "# smiles_column = 'AAM'\n",
    "# target_columns = ['lograte']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform data splitting for training, validation, and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ReactionDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số mẫu để pre-train: 353468\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# --- BƯỚC 1: ĐỊNH NGHĨA DATASET CHO PRE-TRAINING ---\n",
    "\n",
    "class MaskedFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset này sẽ:\n",
    "    1. Tải các features từ file NPZ.\n",
    "    2. Trong mỗi lần lấy dữ liệu (__getitem__), nó sẽ che ngẫu nhiên một phần \n",
    "       của node_attrs và edge_attrs.\n",
    "    3. Trả về cả dữ liệu gốc và dữ liệu đã bị che.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_attrs, edge_attrs, edge_indices, mask_fraction=0.15):\n",
    "        self.node_attrs = [torch.tensor(attrs, dtype=torch.float32) for attrs in node_attrs]\n",
    "        self.edge_attrs = [torch.tensor(attrs, dtype=torch.float32) for attrs in edge_attrs]\n",
    "        self.edge_indices = [torch.tensor(idx, dtype=torch.long) for idx in edge_indices]\n",
    "        self.mask_fraction = mask_fraction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.node_attrs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Lấy dữ liệu gốc\n",
    "        original_nodes = self.node_attrs[idx]\n",
    "        original_edges = self.edge_attrs[idx]\n",
    "        edge_index = self.edge_indices[idx]\n",
    "\n",
    "        # Tạo bản sao để che\n",
    "        masked_nodes = original_nodes.clone()\n",
    "        masked_edges = original_edges.clone()\n",
    "\n",
    "        # Che ngẫu nhiên một phần node features\n",
    "        num_node_features_to_mask = int(original_nodes.shape[0] * self.mask_fraction)\n",
    "        node_mask_indices = torch.randperm(original_nodes.shape[0])[:num_node_features_to_mask]\n",
    "        masked_nodes[node_mask_indices] = 0.0 # Che bằng cách gán giá trị 0\n",
    "\n",
    "        # Che ngẫu nhiên một phần edge features\n",
    "        num_edge_features_to_mask = int(original_edges.shape[0] * self.mask_fraction)\n",
    "        edge_mask_indices = torch.randperm(original_edges.shape[0])[:num_edge_features_to_mask]\n",
    "        masked_edges[edge_mask_indices] = 0.0 # Che bằng cách gán giá trị 0\n",
    "        \n",
    "        return {\n",
    "            \"masked_nodes\": masked_nodes,\n",
    "            \"masked_edges\": masked_edges,\n",
    "            \"original_nodes\": original_nodes,\n",
    "            \"original_edges\": original_edges,\n",
    "            \"node_mask_indices\": node_mask_indices,\n",
    "            \"edge_mask_indices\": edge_mask_indices,\n",
    "            \"edge_index\": edge_index\n",
    "        }\n",
    "\n",
    "# --- TẢI VÀ KẾT HỢP TẤT CẢ DỮ LIỆU ĐỂ PRE-TRAIN ---\n",
    "# Pre-training là tự giám sát nên chúng ta có thể dùng cả train/val/test data\n",
    "\n",
    "# Tải dữ liệu từ các file NPZ của bạn\n",
    "train_npz = np.load(f'../chemprop/data/RC/full/barriers_rgd1/barriers_rgd1_aam_train_rc_processed_data.npz', allow_pickle=True)\n",
    "val_npz = np.load(f'../chemprop/data/RC/full/barriers_rgd1/barriers_rgd1_aam_val_rc_processed_data.npz', allow_pickle=True)\n",
    "test_npz = np.load(f'../chemprop/data/RC/full/barriers_rgd1/barriers_rgd1_aam_test_rc_processed_data.npz', allow_pickle=True)\n",
    "\n",
    "# Kết hợp dữ liệu\n",
    "all_node_attrs = np.concatenate((train_npz['node_attrs'], val_npz['node_attrs'], test_npz['node_attrs']))\n",
    "all_edge_attrs = np.concatenate((train_npz['edge_attrs'], val_npz['edge_attrs'], test_npz['edge_attrs']))\n",
    "all_edge_indices = np.concatenate((train_npz['edge_indices'], val_npz['edge_indices'], test_npz['edge_indices']))\n",
    "\n",
    "print(f\"Tổng số mẫu để pre-train: {len(all_node_attrs)}\")\n",
    "\n",
    "def get_reverse_edge_index(edge_index):\n",
    "    \"\"\"Tính toán chỉ số của các cạnh ngược.\"\"\"\n",
    "    rev_edge_index = torch.zeros_like(edge_index[0])\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        # Tìm cạnh ngược (j, i) cho mỗi cạnh (i, j)\n",
    "        edge_to_find = edge_index[:, i].flip(0)\n",
    "        # So sánh để tìm vị trí\n",
    "        matches = (edge_index.T == edge_to_find).all(dim=1)\n",
    "        # Lấy chỉ số đầu tiên tìm thấy\n",
    "        rev_idx = torch.where(matches)[0]\n",
    "        if rev_idx.numel() > 0:\n",
    "            rev_edge_index[i] = rev_idx[0]\n",
    "        else:\n",
    "            # Xử lý trường hợp không tìm thấy cạnh ngược (ít khả năng xảy ra nếu đồ thị là vô hướng)\n",
    "            rev_edge_index[i] = -1 # hoặc một giá trị đặc biệt\n",
    "    return rev_edge_index\n",
    "\n",
    "\n",
    "def collate_pretraining_batch(samples):\n",
    "    \"\"\"\n",
    "    Hàm này sẽ gộp một list các sample (dictionaries) thành một batch duy nhất,\n",
    "    đồng thời đảm bảo các thuộc tính V, E, và rev_edge_index được đặt tên đúng.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        edge_index = sample[\"edge_index\"]\n",
    "        \n",
    "        # *** THÊM BƯỚC NÀY ***\n",
    "        # Tính toán rev_edge_index\n",
    "        rev_edge_index = get_reverse_edge_index(edge_index)\n",
    "\n",
    "        data_point = Data(\n",
    "            V=sample[\"masked_nodes\"],\n",
    "            E=sample[\"masked_edges\"],\n",
    "            edge_index=edge_index,\n",
    "            \n",
    "            # Thêm rev_edge_index vào đối tượng Data\n",
    "            rev_edge_index=rev_edge_index,\n",
    "            \n",
    "            original_V=sample[\"original_nodes\"],\n",
    "            node_mask_indices=sample[\"node_mask_indices\"],\n",
    "            sample_idx=i\n",
    "        )\n",
    "        batch_data.append(data_point)\n",
    "\n",
    "    return Batch.from_data_list(batch_data)\n",
    "\n",
    "# Khởi tạo lại DataLoader với collate_fn mới\n",
    "pretrain_dataset = MaskedFeatureDataset(all_node_attrs, all_edge_attrs, all_edge_indices)\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True, collate_fn=collate_pretraining_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch_geometric.loader import DataLoader \n",
    "# from torch_geometric.data import Data, Batch\n",
    "\n",
    "# # --- ĐỊNH NGHĨA DATASET MỚI CHO DGI ---\n",
    "\n",
    "# class GraphDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Dataset đơn giản cho DGI:\n",
    "#     1. Tải các features từ file NPZ.\n",
    "#     2. Chỉ trả về dữ liệu đồ thị gốc, không che (masking).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, node_attrs, edge_attrs, edge_indices):\n",
    "#         self.node_attrs = [torch.tensor(attrs, dtype=torch.float32) for attrs in node_attrs]\n",
    "#         self.edge_attrs = [torch.tensor(attrs, dtype=torch.float32) for attrs in edge_attrs]\n",
    "#         self.edge_indices = [torch.tensor(idx, dtype=torch.long) for idx in edge_indices]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.node_attrs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Lọc bỏ các đồ thị rỗng ngay tại đây\n",
    "#         if self.node_attrs[idx].shape[0] == 0:\n",
    "#             # Trả về một đồ thị rỗng hợp lệ thay vì None hoặc dictionary\n",
    "#             return Data()\n",
    "\n",
    "#         nodes = self.node_attrs[idx]\n",
    "#         edges = self.edge_attrs[idx]\n",
    "#         edge_index = self.edge_indices[idx]\n",
    "        \n",
    "#         # (Tùy chọn nhưng khuyến khích) Tính rev_edge_index ở đây nếu cần\n",
    "#         rev_edge_index = get_reverse_edge_index(edge_index)\n",
    "\n",
    "#         # Tạo và trả về một đối tượng Data hoàn chỉnh\n",
    "#         data_point = Data(\n",
    "#             V=nodes,\n",
    "#             E=edges,\n",
    "#             edge_index=edge_index,\n",
    "#             rev_edge_index=rev_edge_index\n",
    "#         )\n",
    "#         return data_point\n",
    "\n",
    "# # --- TẢI DỮ LIỆU (Giữ nguyên) ---\n",
    "# train_npz = np.load(f'../chemprop/data/normal/barriers_cycloadd/barriers_cycloadd_aam_train_processed_data.npz', allow_pickle=True)\n",
    "# val_npz = np.load(f'../chemprop/data/normal/barriers_cycloadd/barriers_cycloadd_aam_val_processed_data.npz', allow_pickle=True)\n",
    "# test_npz = np.load(f'../chemprop/data/normal/barriers_cycloadd/barriers_cycloadd_aam_test_processed_data.npz', allow_pickle=True)\n",
    "\n",
    "# all_node_attrs = np.concatenate((train_npz['node_attrs'], val_npz['node_attrs'], test_npz['node_attrs']))\n",
    "# all_edge_attrs = np.concatenate((train_npz['edge_attrs'], val_npz['edge_attrs'], test_npz['edge_attrs']))\n",
    "# all_edge_indices = np.concatenate((train_npz['edge_indices'], val_npz['edge_indices'], test_npz['edge_indices']))\n",
    "\n",
    "# print(f\"Tổng số mẫu để pre-train: {len(all_node_attrs)}\")\n",
    "\n",
    "# # --- ĐỊNH NGHĨA COLLATE_FN MỚI CHO DGI ---\n",
    "\n",
    "# # Giữ lại hàm này nếu mô hình BondMessagePassing của bạn cần\n",
    "# def get_reverse_edge_index(edge_index):\n",
    "#     # ... (code hàm này giữ nguyên) ...\n",
    "#     rev_edge_index = torch.zeros_like(edge_index[0])\n",
    "#     for i in range(edge_index.shape[1]):\n",
    "#         edge_to_find = edge_index[:, i].flip(0)\n",
    "#         matches = (edge_index.T == edge_to_find).all(dim=1)\n",
    "#         rev_idx = torch.where(matches)[0]\n",
    "#         if rev_idx.numel() > 0:\n",
    "#             rev_edge_index[i] = rev_idx[0]\n",
    "#         else:\n",
    "#             rev_edge_index[i] = -1\n",
    "#     return rev_edge_index\n",
    "\n",
    "# def collate_dgi_batch(samples):\n",
    "#     \"\"\"\n",
    "#     Hàm collate cho DGI, gộp các sample thành một batch của PyG.\n",
    "#     Sử dụng các key tiêu chuẩn: x, edge_index, edge_attr.\n",
    "#     \"\"\"\n",
    "#     batch_data = []\n",
    "#     for sample in samples:\n",
    "#         # Lọc bỏ các đồ thị rỗng để tránh lỗi\n",
    "#         if sample[\"nodes\"].shape[0] == 0:\n",
    "#             continue\n",
    "            \n",
    "#         edge_index = sample[\"edge_index\"]\n",
    "#         rev_edge_index = get_reverse_edge_index(edge_index)\n",
    "\n",
    "#         data_point = Data(\n",
    "#             x=sample[\"nodes\"],          # Key tiêu chuẩn của PyG\n",
    "#             edge_attr=sample[\"edges\"],  # Key tiêu chuẩn của PyG\n",
    "#             edge_index=edge_index,\n",
    "#             rev_edge_index=rev_edge_index\n",
    "#         )\n",
    "#         batch_data.append(data_point)\n",
    "\n",
    "#     if not batch_data:\n",
    "#         return None\n",
    "\n",
    "#     return Batch.from_data_list(batch_data)\n",
    "\n",
    "# # --- KHỞI TẠO DATALOADER MỚI ---\n",
    "# pretrain_dataset = GraphDataset(all_node_attrs, all_edge_attrs, all_edge_indices)\n",
    "# pretrain_loader = DataLoader(\n",
    "#     pretrain_dataset, \n",
    "#     batch_size=32, \n",
    "#     shuffle=True, \n",
    "#     collate_fn=collate_dgi_batch,\n",
    "#     num_workers=0 # Bắt đầu với 0 để gỡ lỗi\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn \n",
    "# import pytorch_lightning as pl\n",
    "# from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# # --- CÁC THÀNH PHẦN CỦA DGI (ĐÃ SỬA) ---\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     \"\"\"Bộ phân biệt cho DGI.\"\"\"\n",
    "#     def __init__(self, hidden_dim):\n",
    "#         super().__init__()\n",
    "#         self.weight = torch.nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "#         torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "#     def forward(self, node_summary, graph_summary):\n",
    "#         graph_summary_proj = torch.matmul(graph_summary, self.weight)\n",
    "#         return torch.sum(node_summary * graph_summary_proj, dim=1)\n",
    "\n",
    "\n",
    "# class DeepGraphInfomax(pl.LightningModule):\n",
    "#     \"\"\"\n",
    "#     Mô hình Lightning cho Deep Graph Infomax.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, gnn_encoder):\n",
    "#         super().__init__()\n",
    "#         self.gnn = gnn_encoder\n",
    "#         # Giả định encoder của bạn có thuộc tính .output_dim\n",
    "#         # Nếu không, hãy thay thế bằng hidden_dim bạn đã định nghĩa\n",
    "#         hidden_dim = self.gnn.output_dim \n",
    "        \n",
    "#         self.discriminator = Discriminator(hidden_dim)\n",
    "#         # SỬA Ở ĐÂY: Dùng nn.BCEWithLogitsLoss() từ torch.nn\n",
    "#         self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         # Giả định mô hình gnn của bạn có thể nhận batch của PyG\n",
    "#         # Chemprop MPNN thường nhận batch và tự lấy V, E, rev_edge_index\n",
    "#         node_embeddings = self.gnn(batch) \n",
    "        \n",
    "#         # Lấy biểu diễn đồ thị (global summary)\n",
    "#         graph_summary = global_mean_pool(node_embeddings, batch.batch)\n",
    "        \n",
    "#         return node_embeddings, graph_summary\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         if batch is None:\n",
    "#             return None\n",
    "\n",
    "#         node_embeddings, graph_summary = self.forward(batch)\n",
    "\n",
    "#         positive_graph_summary = graph_summary[batch.batch]\n",
    "        \n",
    "#         shuffled_graph_summary = torch.roll(graph_summary, shifts=1, dims=0)\n",
    "#         negative_graph_summary = shuffled_graph_summary[batch.batch]\n",
    "\n",
    "#         positive_score = self.discriminator(node_embeddings, positive_graph_summary)\n",
    "#         negative_score = self.discriminator(node_embeddings, negative_graph_summary)\n",
    "\n",
    "#         loss_pos = self.loss_fn(positive_score, torch.ones_like(positive_score))\n",
    "#         loss_neg = self.loss_fn(negative_score, torch.zeros_like(negative_score))\n",
    "        \n",
    "#         loss = loss_pos + loss_neg\n",
    "        \n",
    "#         acc = ( (positive_score > 0).float().sum() + (negative_score < 0).float().sum() ) / (2 * len(positive_score))\n",
    "#         self.log_dict({'train_loss': loss, 'train_acc': acc}, prog_bar=True)\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Message-Passing Neural Network (MPNN) inputs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing\n",
    "\n",
    "Message passing blocks must be given the shape of the featurizer's outputs.\n",
    "\n",
    "Options are `mp = nn.BondMessagePassing()` or `mp = nn.AtomMessagePassing()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct MPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SỬA LẠI LỚP PRETRAININGMPNN\n",
    "\n",
    "# from torch.nn import Linear, MSELoss\n",
    "\n",
    "# class PretrainingMPNN(pl.LightningModule):\n",
    "#     def __init__(self, message_passing, node_feature_dim):\n",
    "#         super().__init__()\n",
    "#         self.message_passing = message_passing\n",
    "#         hidden_dim = self.message_passing.output_dim\n",
    "#         self.node_prediction_head = Linear(hidden_dim, node_feature_dim)\n",
    "#         self.loss_fn = MSELoss()\n",
    "\n",
    "# # Sửa trong lớp PretrainingMPNN\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         # Hàm forward của BondMessagePassing sẽ tự động dùng batch.V và batch.E\n",
    "        \n",
    "#         # SỬA Ở ĐÂY: Gán kết quả cho một biến duy nhất\n",
    "#         node_embeddings = self.message_passing(batch)\n",
    "        \n",
    "#         predicted_node_features = self.node_prediction_head(node_embeddings)\n",
    "#         return predicted_node_features\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         predicted_nodes = self.forward(batch)\n",
    "        \n",
    "#         # Tạo boolean mask cho toàn bộ batch\n",
    "#         total_nodes = batch.V.shape[0] # Dùng batch.V thay vì batch.x\n",
    "#         node_mask = torch.zeros(total_nodes, dtype=torch.bool, device=self.device)\n",
    "\n",
    "#         base_idx = 0\n",
    "#         for i in range(batch.num_graphs):\n",
    "#             # Lấy các chỉ số mask cho từng đồ thị con trong batch\n",
    "#             sample_mask_indices = batch.node_mask_indices[batch.batch == i]\n",
    "            \n",
    "#             # Cập nhật mask tổng\n",
    "#             node_mask[base_idx + sample_mask_indices] = True\n",
    "            \n",
    "#             # Di chuyển đến điểm bắt đầu của đồ thị tiếp theo\n",
    "#             num_nodes_in_sample = (batch.batch == i).sum()\n",
    "#             base_idx += num_nodes_in_sample\n",
    "        \n",
    "#         # Chỉ tính loss trên các node đã bị che\n",
    "#         if node_mask.sum() > 0:\n",
    "#             # Dùng batch.original_V để lấy feature gốc\n",
    "#             loss = self.loss_fn(predicted_nodes[node_mask], batch.original_V[node_mask])\n",
    "#             self.log(\"train_loss\", loss, prog_bar=True)\n",
    "#             return loss\n",
    "            \n",
    "#         return None\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import MSELoss\n",
    "from lightning import pytorch as pl\n",
    "\n",
    "from chemprop.data import BatchMolGraph\n",
    "from chemprop.nn import MessagePassing\n",
    "from chemprop.schedulers import build_NoamLike_LRSched\n",
    "\n",
    "\n",
    "class Pretrainable_MPNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Phiên bản MPNN được chỉnh sửa cho tác vụ pre-train (tái tạo đặc trưng nút).\n",
    "    Lớp này chỉ giữ lại bộ mã hóa MessagePassing và thêm vào một đầu dự đoán nút.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MessagePassing,\n",
    "        node_feature_dim: int, # THÊM: Cần biết chiều của đặc trưng nút để tái tạo\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 1e-4,\n",
    "        max_lr: float = 1e-3,\n",
    "        final_lr: float = 1e-4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"message_passing\"])\n",
    "        hidden_dim = message_passing.output_dim\n",
    "\n",
    "        # === SỬA: CÁC KHỐI KIẾN TRÚC ===\n",
    "        self.message_passing = message_passing\n",
    "        \n",
    "        # THÊM: Đầu dự đoán để tái tạo lại đặc trưng nút từ các biểu diễn đã học\n",
    "        self.node_prediction_head = nn.Linear(hidden_dim, node_feature_dim)\n",
    "        \n",
    "        # THÊM: Hàm loss để so sánh nút dự đoán và nút gốc\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "        # BỎ: GatedSkipBlock, GRUCell, Predictor, Metrics đã được loại bỏ\n",
    "\n",
    "    def forward(self, batch: BatchMolGraph) -> Tensor:\n",
    "        \"\"\"\n",
    "        SỬA: Hàm forward giờ đây chỉ thực hiện việc mã hóa và dự đoán nút.\n",
    "        Nó nhận vào batch dữ liệu đã được che (masked).\n",
    "        \"\"\"\n",
    "        # 1. Chạy Message Passing để học đặc trưng nguyên tử từ input đã bị che\n",
    "        #    message_passing sẽ tự động dùng batch.V, batch.E, ...\n",
    "        node_embeddings = self.message_passing(batch)\n",
    "        \n",
    "        # 2. Dùng đầu dự đoán để tái tạo lại đặc trưng gốc của các nút\n",
    "        predicted_node_features = self.node_prediction_head(node_embeddings)\n",
    "        \n",
    "        return predicted_node_features\n",
    "\n",
    "    def training_step(self, batch: BatchMolGraph, batch_idx):\n",
    "        \"\"\"\n",
    "        SỬA: training_step được viết lại hoàn toàn cho tác vụ pre-train.\n",
    "        \"\"\"\n",
    "        # 1. Thực hiện forward pass để lấy các nút đã được dự đoán/tái tạo\n",
    "        predicted_nodes = self(batch)\n",
    "        \n",
    "        # 2. Tạo một mask để xác định các nút nào đã bị che trong toàn bộ batch\n",
    "        total_nodes = batch.V.shape[0]\n",
    "        node_mask = torch.zeros(total_nodes, dtype=torch.bool, device=self.device)\n",
    "\n",
    "        base_idx = 0\n",
    "        for i in range(batch.num_graphs):\n",
    "            # Lấy chỉ số các nút bị che của từng đồ thị trong batch\n",
    "            sample_mask_indices = batch.node_mask_indices[batch.batch == i]\n",
    "            # Cập nhật vào mask chung của toàn batch\n",
    "            node_mask[base_idx + sample_mask_indices] = True\n",
    "            # Di chuyển đến điểm bắt đầu của đồ thị tiếp theo\n",
    "            num_nodes_in_sample = (batch.batch == i).sum()\n",
    "            base_idx += num_nodes_in_sample\n",
    "        \n",
    "        # 3. Chỉ tính loss trên các nút đã bị che\n",
    "        if node_mask.sum() > 0:\n",
    "            # Lấy các đặc trưng gốc từ batch (batch.original_V)\n",
    "            original_nodes = batch.original_V\n",
    "            loss = self.loss_fn(predicted_nodes[node_mask], original_nodes[node_mask])\n",
    "            self.log(\"train_loss\", loss, prog_bar=True, batch_size=batch.num_graphs)\n",
    "            return loss\n",
    "            \n",
    "        return None # Bỏ qua batch nếu không có nút nào bị che\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Giữ nguyên logic tối ưu hóa\n",
    "        opt = optim.Adam(self.parameters(), self.hparams.init_lr)\n",
    "        \n",
    "        if self.trainer is None or self.trainer.train_dataloader is None:\n",
    "            return {\"optimizer\": opt}\n",
    "            \n",
    "        steps_per_epoch = self.trainer.num_training_batches\n",
    "        warmup_steps = self.hparams.warmup_epochs * steps_per_epoch\n",
    "\n",
    "        if self.trainer.max_epochs == -1:\n",
    "            cooldown_steps = 100 * warmup_steps\n",
    "        else:\n",
    "            cooldown_epochs = self.trainer.max_epochs - self.hparams.warmup_epochs\n",
    "            cooldown_steps = cooldown_epochs * steps_per_epoch\n",
    "            \n",
    "        lr_sched = build_NoamLike_LRSched(\n",
    "            opt,\n",
    "            warmup_steps,\n",
    "            cooldown_steps,\n",
    "            self.hparams.init_lr,\n",
    "            self.hparams.max_lr,\n",
    "            self.hparams.final_lr\n",
    "        )\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": lr_sched, \"interval\": \"step\"}}\n",
    "\n",
    "    # BỎ: validation_step, test_step, predict_step vì mục tiêu chỉ là pre-train\n",
    "    # BỎ: Các phương thức load_from_checkpoint phức tạp có thể được đơn giản hóa nếu cần"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/labhhc2/anaconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type               | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | message_passing      | BondMessagePassing | 202 K  | train\n",
      "1 | node_prediction_head | Linear             | 6.9 K  | train\n",
      "2 | loss_fn              | MSELoss            | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "209 K     Trainable params\n",
      "0         Non-trainable params\n",
      "209 K     Total params\n",
      "0.839     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/labhhc2/anaconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu Pre-training...\n",
      "Epoch 0:   2%|▏         | 225/11046 [00:02<02:18, 78.16it/s, v_num=21, train_loss=7.41e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labhhc2/anaconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 11046/11046 [36:12<00:00,  5.09it/s, v_num=21, train_loss=5.74e-5] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 11046/11046 [36:12<00:00,  5.08it/s, v_num=21, train_loss=5.74e-5]\n",
      "Pre-training hoàn tất!\n",
      "Encoder đã pre-train được lưu tại: pretrained_dmpnn_nhap_encoder.pt\n"
     ]
    }
   ],
   "source": [
    "# --- BƯỚC 3: KHỞI TẠO VÀ HUẤN LUYỆN MÔ HÌNH PRE-TRAINING (ĐÃ SỬA LỖI) ---\n",
    "from chemprop.nn import BondMessagePassing\n",
    "# Lấy thông số chiều từ dữ liệu\n",
    "node_feature_dim = pretrain_dataset.node_attrs[0].shape[1]\n",
    "edge_feature_dim = pretrain_dataset.edge_attrs[0].shape[1]\n",
    "fdims = (node_feature_dim, edge_feature_dim)\n",
    "\n",
    "# Khởi tạo encoder D-MPNN\n",
    "mp_encoder = BondMessagePassing(*fdims)\n",
    "\n",
    "# Khởi tạo mô hình pre-training\n",
    "# SỬA Ở ĐÂY: Xóa `edge_feature_dim` khỏi lời gọi hàm\n",
    "pretrain_model = Pretrainable_MPNN(\n",
    "    message_passing=mp_encoder, \n",
    "    node_feature_dim=node_feature_dim\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer của PyTorch Lightning\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50, \n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    logger=True\n",
    ")\n",
    "\n",
    "# Bắt đầu pre-training!\n",
    "print(\"Bắt đầu Pre-training...\")\n",
    "trainer.fit(pretrain_model, pretrain_loader)\n",
    "print(\"Pre-training hoàn tất!\")\n",
    "\n",
    "\n",
    "# --- BƯỚC 4: LƯU LẠI ENCODER ĐÃ ĐƯỢC HUẤN LUYỆN ---\n",
    "output_path = \"pretrained_dmpnn_nhap_encoder.pt\"\n",
    "torch.save(pretrain_model.message_passing.state_dict(), output_path)\n",
    "print(f\"Encoder đã pre-train được lưu tại: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- HUẤN LUYỆN MÔ HÌNH DGI ---\n",
    "# from chemprop import nn\n",
    "\n",
    "# # Lấy thông số chiều từ dữ liệu\n",
    "# node_feature_dim = pretrain_dataset.node_attrs[0].shape[1]\n",
    "# edge_feature_dim = pretrain_dataset.edge_attrs[0].shape[1]\n",
    "# fdims = (node_feature_dim, edge_feature_dim)\n",
    "# hidden_dim = 300 # Ví dụ\n",
    "\n",
    "# mp_encoder = nn.BondMessagePassing(*fdims, d_h=hidden_dim)\n",
    "\n",
    "# # Khởi tạo mô hình DGI với encoder GNN\n",
    "# pretrain_model = DeepGraphInfomax(gnn_encoder=mp_encoder)\n",
    "\n",
    "# # Khởi tạo Trainer (giữ nguyên)\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=50, \n",
    "#     accelerator=\"auto\",\n",
    "#     devices=1\n",
    "# )\n",
    "\n",
    "# # Bắt đầu pre-training!\n",
    "# print(\"Bắt đầu Pre-training với Deep Graph Infomax...\")\n",
    "# trainer.fit(pretrain_model, pretrain_loader)\n",
    "# print(\"Pre-training hoàn tất!\")\n",
    "\n",
    "# # --- LƯU LẠI ENCODER ---\n",
    "# output_path = \"pretrained_dgi_encoder.pt\"\n",
    "# # Lưu lại gnn encoder, vì đó là phần chúng ta cần cho downstream task\n",
    "# torch.save(pretrain_model.gnn.state_dict(), output_path)\n",
    "# print(f\"Encoder đã pre-train được lưu tại: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
